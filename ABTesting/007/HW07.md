# ДЗ 7

## Задание

На основе выбранных продуктов из первого вебинара, выберите 2 A/B-теста и составьте план эксперимента, чтобы избежать Peeking Problem

Распишите, какие метрики и как вы будете анализировать

## Решение

__Peeking problem__ (проблема подглядывания) проявляется, когда вы проверяете промежуточные результаты с готовностью принять решение: раскатить одну из версий, если различие между тестом и контролем окажется значимым.

Изучим этот вопрос более [подробнее](https://habr.com/ru/companies/uchi_ru/articles/500918/)

Проблема подглядывания (peeking problem) - это частный случай множественного тестирования гипотез.

Смысл в том, что значение p-value по ходу теста может случайно опускаться ниже принятого уровня значимости. Если внимательно следить за экспериментом, то можно поймать такой момент и ошибочно сделать вывод о статистической значимости.

Предположим, что мы отошли от описанной в начале поста схемы проведения тестов и решили подводить итоги на уровне значимости 5% каждый день (или просто больше одного раза за время теста). 

Под подведением итогов я понимаю признание теста положительным, если p-value ниже 0.05, и его продолжение в противном случае. При такой стратегии доля ложноположительных результатов будет пропорциональна количеству проверок и уже за первый месяц достигнет 28%. Такая огромная разница кажется контринтуитивной, поэтому обратимся к методике A/A-тестов, незаменимой для разработки схем A/B-тестирования.

Идея A/A-теста проста: симулировать на исторических данных много A/B-тестов со случайным разбиением на группы. Разницы между группами заведомо нет, поэтому можно точно оценить долю ошибок первого рода в своей схеме A/B-тестирования. На гифке ниже показано, как изменяются значения p-value по дням для четырёх таких тестов. Равный 0.05 уровень значимости обозначен пунктирной линией. Когда p-value опускается ниже, мы окрашиваем график теста в красный. Если бы в этом время подводились итоги теста, он был бы признан успешным.

![Эволюция значений](/ABTesting/Pictures/007_051.gif)

Рассчитаем аналогично 10 тысяч A/A-тестов продолжительностью в один месяц и сравним доли ложноположительных результатов в схеме с подведением итогов в конце срока и каждый день. 

Для наглядности приведём графики блуждания p-value по дням для первых 100 симуляций. 

Каждая линия — p-value одного теста, красным выделены траектории тестов, в итоге ошибочно признанных удачными (чем меньше, тем лучше), пунктирная линия — требуемое значение p-value для признания теста успешным.

![Эволюция значений](/ABTesting/Pictures/007_052.png)

На графике можно насчитать 7 ложноположительных тестов, а всего среди 10 тысяч их было 502, или 5%. Хочется отметить, что p-value многих тестов по ходу наблюдений опускались ниже 0.05, но к концу наблюдений выходили за пределы уровня значимости. Теперь оценим схему тестирования с подведением итогов каждый день:

![Эволюция значений](/ABTesting/Pictures/007_053.png)

Красных линий настолько много, что уже ничего не понятно. Перерисуем, обрывая линии тестов, как только их p-value достигнут критического значения:

![Эволюция значений](/ABTesting/Pictures/007_054.png)

Всего будет 2813 ложноположительных тестов из 10 тысяч, или 28%. Понятно, что такая схема нежизнеспособна.

Хоть проблема подглядывания — это частный случай множественного тестирования, применять стандартные поправки (Бонферрони и другие) здесь не стоит, потому что они окажутся излишне консервативными. На графике ниже — доля ложноположительных результатов в зависимости от количества тестируемых групп (красная линия) и количества подглядываний (зелёная линия).

![Эволюция значений](/ABTesting/Pictures/007_055.png)

Хотя на бесконечности и в подглядываниях мы вплотную приблизимся к 1, доля ошибок растёт гораздо медленнее. Это объясняется тем, что сравнения в этом случае независимыми уже не являются.

Добавим ещё [знаний](https://gopractice.ru/data/how-not-to-analyze-abtests/)

## Peeking problem или проблема подглядывания

Применение стандартных критериев в рамках частотного подхода к статистике (Хи-квадрат, критерий Стюдента), которые используются для расчета p-value и статистической значимости, требуют выполнения различных условий. Например, многие критерии подразумевают нормальное распределение изучаемой величины.

Но есть еще одно важное условие, о существовании которого многие забывают: размер выборки для эксперимента должен быть определен заранее.

Вы должны заранее решить, сколько наблюдений хотите собрать. Потом посчитать результаты и принять решение. Если вдруг выявить значимую разницу на собранном количестве данных не получилось, то продолжать эксперимент с целью сбора дополнительных наблюдений нельзя. Можно только запустить тест заново.

Описанная логика звучит странно в контексте A/B тестов в интернете, где можно смотреть на результаты в режиме реального времени, где добавление новых пользователей в эксперимент ничего не стоит.

Дело в том, что используемый для A/B тестов математический аппарат в рамках частотного подхода к статистике разрабатывался задолго до появления интернета. Тогда большинство прикладных задач подразумевало фиксированный и заранее определенный размер выборки для проверки гипотезы.

Интернет поменял парадигму A/B тестирования. Вместо выбора фиксированного размера выборки перед запуском эксперимента большинство предпочитают собирать данные, пока разница между тестом и контролем не станет значимой. Следствием такого изменения в процедуре проведения экспериментов стало то, что расчеты p-value старыми способами перестали работать. Реальное p-value при регулярной проверке результатов теста становится намного больше, чем то p-value, который вы получаете, используя обычные статистические критерии, которые при такой процедуре перестают работать.

Проблема подглядывания проявляется, когда вы проверяете промежуточные результаты с готовностью принять решение: раскатить одну из версий, если различие между тестом и контролем окажется значимым. Если вы зафиксировали размер выборки и просто наблюдаете за результатами в процессе набора наблюдений (и ничего не делаете на их основе), а потом принимаете решение, когда набралось нужное количество данных, то никаких проблем не возникает.

Правильная процедура A/B тестирования (в рамках частотного подхода):

![Эволюция значений](/ABTesting/Pictures/007_056.PNG)

Неправильная процедура A/B тестирования:

![Эволюция значений](/ABTesting/Pictures/007_057.PNG)

## Почему подглядывания увеличивают p-value
Давайте вернемся к эксперименту про конверсию в первую покупку в игре. Предположим, мы знаем, что в реальности сделанные изменения не оказали никакого влияния.

Ниже изображена динамика разницы конверсий в покупку между тестовой и контрольной версиями продукта (синяя линия). Зеленая и красная линия отражают границы диапазона неразличимости при условии, что заранее было выбрано соответствующее количество наблюдений.

![Эволюция значений](/ABTesting/Pictures/007_058.PNG)

При правильном процессе A/B тестирования надо заранее определить количество пользователей, на основе которых будет оцениваться результат, собрать наблюдения, посчитать результаты и сделать вывод. Такая процедура гарантирует, что при многократном ее повторении в 95% случаев критерий не увидит разницы между одинаковыми версиями (при соответствующем уровне доверия).

Все меняется, если вы начинаете проверять результаты с определенной частотой и готовы действовать на основе наблюдаемых различий. В таком случае вместо вопроса о том, является ли разница значимой в определенный заранее выбранный момент в будущем, вы спрашиваете, выходит ли разница за диапазон неразличимости хотя бы раз в процессе сбора данных. Это два совершенно разных вопроса.

Даже если две группы идентичны, то разница конверсий может периодически выходить за границы зоны неразличимости по мере накопления наблюдений. Это совершенно нормально, так как границы сформированы так, чтобы при тестировании одинаковых версий лишь в 95% случаев разница оказывалась в их пределах.

Поэтому при регулярной проверке результатов в процессе проведения теста с готовностью принять решение при наличии значимой разницы вы начинаете кумулятивно накапливать возможные случайные моменты, когда разница выходит за пределы диапазона. Следствие — p-value растет с каждой новой проверкой.

Вот [наглядный пример](https://gopractice.ru/goto/http://varianceexplained.org/r/bayesian-ab-testing/) того, как именно подглядывания увеличивают p-value.

## Влияние подглядываний на p-value
Чем чаще вы смотрите на промежуточные результаты A/B теста с готовностью принять на их основе решение, тем выше становится вероятность, что критерий покажет значимую разницу, когда ее нет:

+ 2 подглядывания с готовностью принять решение о завершении теста увеличивают p-value в 2 раза;
+ 5 подглядываний в 3.2 раза;
+ 10 000 подглядываний более чем в 12 раз.

![Эволюция значений](/ABTesting/Pictures/007_059.PNG)

## Варианты решения проблемы подглядывания:

+ Фиксировать размер выборки заранее и не проверять результаты, пока все данные не собраны. <br>
Очень правильный и очень непрактичный подход. Если эксперимент не даст никакого сигнала, то придется все начинать заново.

### Математические пути решения проблемы:
+ Sequential experiment design,
+ байесовский подход к A/B тестированию,
+ снижение чувствительности критерия

Проблема подглядывания может быть решена математическим путем. Например, Optimizely и Google Experiments используют для ее решения микс байесовского подхода к A/B тестированию и Sequential experiment design. 

Для сервисов вроде Optimizely — это необходимость, так как их ценность сводится к тому, что они определяют лучший вариант на основе регулярной проверки результатов A/B теста на лету. 

Подробнее можно почитать по следующим ссылкам: [Optimizely](https://gopractice.ru/goto/https://www.optimizely.com/contentassets/9205a8a811e84957a7cca527d4af20be/whitepaper_optimizely_stats_engine.pdf/download), [Google](https://gopractice.ru/goto/http://blog.analytics-toolkit.com/2017/bayesian-ab-testing-not-immune-to-optional-stopping-issues/).

Продуктовый подход с мягким обязательством по времени теста и коррекцией на суть проблемы подглядывания:

+ При работе над продуктом ваша цель — получить сигнал, необходимый для принятия решения. Описанная ниже логика неидеальна с математической точки зрения, но решает продуктовую задачу.

Суть подхода сводится к тому, чтобы предварительно оценить необходимую выборку для выявления эффекта в A/B тесте и учесть природу проблемы подглядывания в процессе промежуточных проверок. Это позволяет минимизировать негативные последствия при анализе результатов.

Перед стартом эксперимента стоит оценить, какая нужна выборка, чтобы с приемлемой вероятностью увидеть изменение, если это изменение в реальности есть.

Это полезное упражнение безотносительно проблемы подглядывания, так как для некоторых продуктовых фич требуется столь большая выборка, чтобы идентифицировать их эффект, что тестировать их на текущем этапе развития продукта нет смысла.

Держа в уме посчитанную ранее выборку, после запуска эксперимента можно (и даже нужно) следить за динамикой изменений, но не принимать решений, при первом выходе разницы в зону значимости.

Нужно продолжать наблюдать. Если разница зафиксируется, то, скорее всего, влияние есть. Если же станет вновь неразличимой, то сделать однозначного вывода об улучшении нельзя.

Давайте посмотрим на результаты двух экспериментов ниже. Оба эксперимента шли 20 дней, каждая точка на графике — относительная разница в метрике между тестом и контролем на конец соответствующего дня с доверительным интервалом. Если доверительный интервал не пересекает ноль (идентично выполнению условия p-value < x), то разница является значимой (при выборе соответствующей выборки заранее). В этом случае точка выделяется зеленым.

В первом эксперименте, начиная с 6 дня, разница между версиями стала значимой и доверительный интервал больше не пересекал 0. Такая устойчивая картина дает четкий сигнал о том, что с высокой степенью вероятности тестовая версия работает лучше контрольной. Сложностей с интерпретацией результатов нет.

![Эволюция значений](/ABTesting/Pictures/007_060.PNG)

Во втором эксперименте разница иногда выходила в зону значимости, но потом вновь становилась неразличимой. Если бы мы не знали о проблеме подглядывания, то мы закончили бы эксперимент на 4 день, решив, что тестовая версия выиграла. Но предложенный способ визуализации динамики A/B теста во времени показывает, что устойчивой измеримой разницы между группами нет.

![Эволюция значений](/ABTesting/Pictures/007_061.PNG)

Возможны пограничные случаи, когда однозначно интерпретировать результаты сложно. Единого способа их разрешения нет, все зависит от рискованности и стоимости принимаемого решения.

+ Есть обычные эксперименты, где вы тестируете разные реализации фичи или небольшие изменения, и готовы принимать решение с большей степенью риска.
+ Есть дорогие решения, когда вы, например, пытаетесь проверить новый вектор развития продукта. В этом случае стоит уделить анализу и изучению данных больше времени. Иногда можно провести эксперимент еще раз.

Описанная логика может звучать как излишнее упрощение с точки зрения математики, но для продуктовой работы она вполне подходит. Когда в продуктовой работе начинают больше заниматься математикой, чем продуктом, то обычно дело в том, что эффект от изменений слишком мал или отсутствует, а эту проблему математикой не решить.

## В заключение

Ключевая мысль этого материала такова:

Если вы проводите A/B тест и в определенный момент разница стала значимой, то не надо сразу заканчивать эксперимент, считая, что одна из групп выиграла. Продолжайте наблюдать. А лучше заранее выберите размер выборки, соберите наблюдения, а потом на их основе посчитайте результаты.

# Решение

На первом семинаре мы предположили следующие гипотезы:

## Гипотезы для ВК
+ Кол-во пользователей в день
+ Кол-во активных пользователей в день
+ Конверсия в продление в подписку
+ Кол-во сообщений об ошибке
+ Активность на пользователя в мессенджере(сообщения в день, кол-во активных диалогов)
+ Кол-во переходов в сторонние сервисы

### Гипотеза 1:  
Если поощрять пользователей за актив связанный с вк фестом, то кол-во активных пользователей увеличится на 5% <br>
__Что делаем в каждой из групп:__
+ Контрольная группа без изменений,
+ тестовая группа: каждые 10 постов в ленте триггерный пост о активности к вк фесту<br>

__На каких пользователях тестируем:__ мало-активные пользователи<br>
__Метрики:__ Кол-во активных пользователей в день, продажа билетов на вк фест(вспомогательная)

### Гипотеза 2:  
Если сделаем “параллельный импорт” музыки и добавим зарубежные новинки, то конверсия в продление в подписку увеличится на 15%<br>
__Что делаем в каждой из групп:__
+ Контрольная группа без изменений,
+ Тестовая группа: даем пробный период с новыми песнями<br>

__На каких пользователях тестируем:__ те, у кого есть активная подписка<br>
__Метрики:__ конверсия в продление в подписку, конверсия в подписку(вспомогательная)

### Гипотеза 3:  
Если настроить тестирующую систему на основе нейросети марусягпт, то кол-во сообщений об ошибке уменьшится на 20%
__Что делаем в каждой из групп:__
+ Контрольная группа без изменений,
+ тестовая группа: запустим нейросеть на обучение по их данным

__На каких пользователях тестируем:__ на всех пользователях с мобильного приложения<br>
__Метрики:__ кол-во сообщений об ошибке

### Гипотеза 4:  

Если создать лимитированный и эксклюзивный стикерпак за активность в месенджере(кол-во сообщений >= 1000), то активность на пользователя в мессенджере увеличится на 10%<br>
__Что делаем в каждой из групп:__ 
+ Контрольная группа без изменений,
+ тестовая группа: сообщение от вк и приветственный стикер с описаниями условий

На каких пользователях тестируем: всех пользователях<br>
__Метрики:__ активность на пользователя в мессенджере, кол-во переходов в сторонние сервисы(вспомогательная)

### Гипотеза 5:  
Если добавить какую-то активность в вк клипы, то кол-во переходов в сторонние сервисы увеличится на 2%<br>

__Что делаем в каждой из групп:__
+ Контрольная группа без изменений,
+ тестовая группа: запускаем рекламу активности от вк клипов<br>

__На каких пользователях тестируем:__ на всех пользователях с мобильного приложения<br>
__Метрики:__ кол-во переходов в сторонние сервисы, кол-во активных пользователей в сторонних сервисах

