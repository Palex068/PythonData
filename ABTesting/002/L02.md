# A/B тестирование

На этом уроке: 
1.	Разберём, что такое продуктовые гипотезы и зачем они нужны.
2.	Рассмотрим структуру хорошей гипотезы.
3.	Увидим, какие подходы к поиску и генерации гипотез существуют.
4.	Узнаете, зачем и как выбирать наиболее ценные для вашего продукта гипотезы.
5.	Рассмотрим какие виды экспериментов существуют
6.	Обсудим как понять можно ли и нужно ли вам проводить А/Б-тест
7.	Посмотрим, как выглядит процесс A/B тестирования
8.	Познакомимся с кратким описанием Google Optimize и первыми шагами в нем.

## Содержание

+ [A/B тестирование](#ав-тестирование)
+ [A/B/N.. Тестирование](#abn-тестирование)
+ [Ухудшающие эксперименты](#ухудшающие-эксперименты)
+ [Область применимости А/B тестов](#область-применимости-аb-тестов)
+ [Процесс A/B тестирования](#процесс-ab-тестирования)
+ [Краткое описание Google Optimize](#краткое-описание-google-optimize)
+ [Работа на семинаре](#работа-на-семинаре)
+ [Практическое задание (ДЗ)](#практическое-задание)

## Статьи
1. [Как приоритизировать продуктовые гипотезы на основе юнит-экономики: разбираем примеры](#как-приоритизировать-продуктовые-гипотезы-на-основе-юнит-экономики-разбираем-примеры)
    + [Метод оценки задач ICE и его соседи](#метод-оценки-задач-ice-и-его-соседи)
    + [Альтернативный подход](#альтернативный-подход)
2. [Пример работы с методом ICE от менеджера продуктов Google и Microsoft](#пример-работы-с-методом-ice-от-менеджера-продуктов-google-и-microsoft)

[Содержание курса](/ABTesting/README.MD)

<hr>

## Какие виды экспериментов существуют
Среди самых распространенных классических методологий в продуктовом мире можно выделить 4 вида экспериментов.
+ A/B тесты
+ A/A тесты
+ A/B/../N (MVT) тесты
+ Ухудшающие эксперименты

![Введение](/ABTesting/Pictures/002_012.PNG)

## А/В тестирование

![a/b testing](/ABTesting/Pictures/002_001.png)

![Введение](/ABTesting/Pictures/002_013.PNG)

![Введение](/ABTesting/Pictures/002_014.PNG)

Между собой сравниваются два варианта (контроль и тест) чтобы определить какой из вариантов эффективнее.
+ Юзеры делятся случайным образом на две группы и берутся из одной генеральной совокупности.
+ Внешние условия должны одинаково влиять на обе группы.

Между группами должно получаться единственное различие - то изменение, которое мы тестируем.

При правильном дизайне - позволяет нам делать выводы о наличии причинно - следственной связи между внедрением новых изменений в продукте и изменением метрик.

Применяется для проверки гипотез и является одним из наиболее честных способов провалидировать продуктовые гипотезы.

### Для чего нужно?

![ПРоцесс тестирования](/ABTesting/Pictures/002_020.PNG)

Главная цель А/А-теста — показать, можно ли доверять результатам эксперимента, который будет запущен в тех же условиях, но уже с разными вариантами страницы. 

Принцип похож на A/B
+ Юзеры делятся случайным образом на две группы
+ Внешние условия должны одинаково влиять на обе группы

Но при этом продуктовых отличий между группами нет. 

Люди видят одинаковые версии продукта.

Это позволяет отловить баги в системе сплитования юзеров - поскольку если между группами будет значимое различие в метриках - оно может объясняться плохим качеством сплит-системы (отличиями в юзерах), а не продуктовым изменением.

### Пример 

Это лендинг, который тестировала команда Copyhackers в ноябре 2012 года:

![a/b testing](/ABTesting/Pictures/002_002.png)

Через 6 дней система тестирования отметила «победивший» вариант при уровне достоверности 95%. 

Ради точности эксперимент продлили на день – и достигли 99,6% точности:

![a/b testing](/ABTesting/Pictures/002_003.png)

Страница на 24% эффективнее, чем точно такая же? Результат ложноположительный. Еще через 3 дня различия исчезли:

![a/b testing](/ABTesting/Pictures/002_004.png)

Вывод: тест слишком рано вычислил победителя.

![Введение](/ABTesting/Pictures/002_015.PNG)

![Введение](/ABTesting/Pictures/002_016.PNG)

![Введение](/ABTesting/Pictures/002_017.PNG)

![Введение](/ABTesting/Pictures/002_018.PNG)

[Содержание](#содержание)

<hr>

## A/B/N.. Тестирование

[A/B/N.. Тестирование](https://www.optimizely.com/optimization-glossary/abn-testing/)

![a/b testing](/ABTesting/Pictures/002_005.png)

Между собой сравниваются более двух вариантов (первая/вторая/n группа...) чтобы определить какой из вариантов эффективнее.

То же самое что и A/B только сравнивается от 2 - можно протестировать много вариантов за схожий промежуток времени.

За преимущества проверки большего числа вариантов придется заплатить объемом трафика и в некоторых случаях точностью тестов.

Этот вид тестирования сопряжен с проблемой [множественной проверки гипотез](https://habr.com/ru/company/yandex/blog/476826/) - поговорим о ней на следующих занятиях.

![a/b/n testing](/ABTesting/Pictures/002_021.PNG)

[Содержание](#содержание)

<hr>

## Ухудшающие эксперименты

![a/b testing](/ABTesting/Pictures/002_006.jpg)

![a/b testing](/ABTesting/Pictures/002_022.PNG)

Ухудшающий эксперимент — это дешевый и довольно простой способ проверять гипотезы с помощью ухудшения кусков продукта. Сразу может возникнуть вопрос - а зачем это делать? Мы же собираемся улучшать продукт и растить его метрики, а не портить его. Все верно! Суть данного подхода состоит в том, чтобы завалидировать существование зависимостей метрик от определенных параметров продукта для дальнейшей генерации идей и гипотез. Найти способ как гарантированно ухудшить продукт довольно легко, а вот гарантированно его улучшить, к сожалению, нет - ведь иначе команды бы уже давно знали, что им делать.
Таким образом, ухудшая кусок продукта можно быстро посмотреть, а есть ли зависимость в принципе: например, влияет ли замедление сайта/приложения на ключевые метрики или нет.
Если реакция метрик есть, значит команде продукта стоит обратить внимание на целесообразность инвестиций своего времени в работу со скоростью сайта/приложения. Если нет, тестировать другие гипотезы. Довольно подробно на русском метод описан в [статье](https://gopractice.ru/ab-test/). Ограничением для данного подхода может являться, например консервативность руководства, людям необходимо правильно донести его суть.

[Содержание](#содержание)

<hr>

## Область применимости А/B тестов

![a/b testing](/ABTesting/Pictures/002_007.png)

![a/b testing](/ABTesting/Pictures/002_008.png)

У данного инструмента есть множество плюсов - например, при правильном проведении мы можем переносить выводы с выборки на генеральную совокупность, а также проверять наличие причинно-следственной связи между выкаткой новой фичи и изменением метрик продукта. 

Однако, за плюсы АБ-тестов необходимо платить: объемом трафика, затратами на разработку и временем. Перед тем, как выбрать этот инструмент для проверки гипотезы нужно проверить себя по следующим пунктам:
1. В вашем продукте уже достаточно трафика, чтобы собрать необходимое количество наблюдений для принятия решений.<br>
Молодым продуктам, которые запустились только недавно и еще не имеют большого количества юзеров, B2B продуктам - увы, этот инструмент не подойдет. Вам лучше использовать качественные методы исследования. Про расчет необходимой выборки и длительности теста - будем говорить на следующих занятиях.
2. Ожидаемый эффект от вашего изменения очень маленький/затраты на проведение теста не окупают потенциальный эффект.<br>
Чем слабее фича влияет на продукт и ваших пользователей, чем ниже ожидаемый эффект, тем больше нам нужно данных для того, чтобы это влияние зафиксировать. Перед тем как пытаться валидировать c помощью а/б теста незначительные для пользователя изменения - лучше хорошо подумать. На разработку и запуск изменения может быть потрачено много времени, человекочасов, трафика и денег. Эффект может не окупить всех затрат. Если вы не Google, Facebook и т.п., которые на своем гигантском трафике могут завалидировать даже самые микроскопические изменения метрик (например, от покраски кнопок в другой цвет:)) - лучше выбирать для данного вида тестирования только действительно сильные гипотезы. Ведь пока вы тратите время на проверку маленьких изменений - у вас могут валяться и ждать своей очереди действительно важные изменения.

[Видео для поднятия настроения](https://www.youtube.com/watch?v=UTPcFZCdSD4&feature=youtu.be) :)

3. У вас есть инфраструктура/компетенции в команде для проведения и оценки экспериментов.<br>
Многие команды неправильно проводят и оценивают эксперименты(несвоевременное время проведения теста, неправильные настройки для разных групп, тестирование нескольких гипотез одновременно). В результате раскатки 'успешных' фич их ключевые метрики могут стоять на месте или падать в результате действий команды.

Для каких изменений вы можете его применять?

## Направления для применения A/B тестов:
+ Тестирование новых фич в приложениях и нового функционала на сайтах
+ Эксперименты в оперейшнс
+ Эксперименты с дизайном
+ Тестирование алгоритмов
+ Эксперименты с ценообразованием

[Содержание](#содержание)

<hr>

## Процесс A/B тестирования

![a/b testing](/ABTesting/Pictures/002_009.png)

![ПРоцесс тестирования](/ABTesting/Pictures/002_019.PNG)

Возможный процесс:

Весь процесс можно разбить на 4-5 этапов:
1. Планирование эксперимента
    + Определяем проблему(ы)/ цели и генерируем гипотезы, формируем бэклог
    + Приоритезируем их
    + Для самых перспективных - продумываем дизайн эксперимента
2. Подготовка продуктовых изменений
3. Запуск теста 
    + Запускаем тест, сплитуя юзеров
    + Делаем мониторинги(дашборды)
4. Анализ результатов теста
5. Принятие решений

![a/b testing](/ABTesting/Pictures/002_023.PNG)

[Почитать интересную статью](https://gopractice.ru/how_to_increase_the_number_of_successful_experiments/)

[Содержание](#содержание)

<hr>

## Краткое описание Google Optimize

![a/b testing](/ABTesting/Pictures/002_010.png)

Не у всех команд есть своя самописная инфраструктура для сплитования и сбора данных - поэтому на рынке существуют различные готовые инструменты для проведения экспериментов.

Например [Optimize и Optimizely](https://insightwhale.com/google-optimize-vs-optimizely-comparing-a-b-testing-tools/)

поговорим про Optimize от гугла
В чем его плюсы:
+ Интеграция с экосистемой гугла (GA/GTM)
+ Почти No-code инструмент (можно немного сэкономить на разработчиках )

Нюансы:
+ Больше подходит для всяких UX тестов
+ Под капотом байесовские методы статистики
+ Есть платная и бесплатная версии

![a/b testing](/ABTesting/Pictures/002_011.png)

(Сравнение платной и бесплатной версий)

Для более детального ознакомления с ним можно почитать статьи:
+ [Google Optimize Guide: Do A/B Testing for Free](https://cxl.com/blog/google-optimize/)
+ [Google Optimize: как тестировать UX сайта без программиста](https://blog.click.ru/growthhacking/google-optimize-kak-testirovat-ux-sajta-bez-programmista/)

и туториалы / курсы:
+ [Optimize video tutorials](https://support.google.com/optimize/answer/9340015?hl=en)
+ [Learn Google Optimize - Beginners - Free A/B Testing Tool](https://www.udemy.com/course/learn-google-optimize-course/)

## Первые шаги (step by step)

1.	Откройте страницу https://optimize.google.com/
2.	Создайте аккаунт
3.	Нажмите ‘Начать’ (cтраница ‘Создайте проект оптимизации’)
4.	Укажите название и URL
5.	Выберите проект оптимизации, который хотите создать
6.	Создайте варианты сайта для тестирования
7.	Внесите изменения
8.	Настройте аудиторию
9.	Установите связь с Google Analytics
10.	Настройте цели
11.	Проверьте связь страницы с Google Optimize

На этом уроке мы:
+ Рассмотрели какие виды экспериментов существуют
+ Обсудили как понять можно ли и нужно ли вам проводить А/Б-тест
+ Посмотрели, как выглядит процесс A/B тестирования

На следующем занятии мы:
+ поговорим о настройках а/б тестов в веб приложениях
+ запустим и с нуля настроим а/б тест в Google Optimize

[Содержание](#содержание)

<hr>

## Работа на семинаре

### Метрики ВК
+ Кол-во пользователей в день
+ Кол-во активных пользователей в день
+ Конверсия в продление в подписку
+ Кол-во сообщений об ошибке
+ Активность на пользователя в мессенджере(сообщения в день, кол-во активных диалогов)
+ Кол-во переходов в сторонние сервисы

### Гипотеза 1:  
Если поощрять пользователей за актив связанный с вк фестом, то кол-во активных пользователей увеличится на 5% <br>
__Что делаем в каждой из групп:__
+ Контрольная группа без изменений,
+ тестовая группа: каждые 10 постов в ленте триггерный пост о активности к вк фесту<br>

__На каких пользователях тестируем:__ мало-активные пользователи<br>
__Метрики:__ Кол-во активных пользователей в день, продажа билетов на вк фест(вспомогательная)

### Гипотеза 2:  
Если сделаем “параллельный импорт” музыки и добавим зарубежные новинки, то конверсия в продление в подписку увеличится на 15%<br>
__Что делаем в каждой из групп:__
+ Контрольная группа без изменений,
+ Тестовая группа: даем пробный период с новыми песнями<br>

__На каких пользователях тестируем:__ те, у кого есть активная подписка<br>
__Метрики:__ конверсия в продление в подписку, конверсия в подписку(вспомогательная)

### Гипотеза 3:  
Если настроить тестирующую систему на основе нейросети марусягпт, то кол-во сообщений об ошибке уменьшится на 20%
__Что делаем в каждой из групп:__
+ Контрольная группа без изменений,
+ тестовая группа: запустим нейросеть на обучение по их данным

__На каких пользователях тестируем:__ на всех пользователях с мобильного приложения<br>
__Метрики:__ кол-во сообщений об ошибке

### Гипотеза 4:  

Если создать лимитированный и эксклюзивный стикерпак за активность в месенджере(кол-во сообщений >= 1000), то активность на пользователя в мессенджере увеличится на 10%<br>
__Что делаем в каждой из групп:__ 
+ Контрольная группа без изменений,
+ тестовая группа: сообщение от вк и приветственный стикер с описаниями условий

На каких пользователях тестируем: всех пользователях<br>
__Метрики:__ активность на пользователя в мессенджере, кол-во переходов в сторонние сервисы(вспомогательная)

### Гипотеза 5:  
Если добавить какую-то активность в вк клипы, то кол-во переходов в сторонние сервисы увеличится на 2%<br>

__Что делаем в каждой из групп:__
+ Контрольная группа без изменений,
+ тестовая группа: запускаем рекламу активности от вк клипов<br>

__На каких пользователях тестируем:__ на всех пользователях с мобильного приложения<br>
__Метрики:__ кол-во переходов в сторонние сервисы, кол-во активных пользователей в сторонних сервисах

[Содержание](#содержание)

<hr>

## Практическое задание

### Задание 1 
Сделайте приоритезацию гипотез из предыдущего урока с помощью ICE

### Задание 2.
Составьте шаблон дизайна эксперимента для гипотезы, которая набрала больше всего баллов в практическом задании предыдущего урока

[Содержание](#содержание)

<hr>

# Как приоритизировать продуктовые гипотезы на основе юнит-экономики: разбираем примеры

О чем статья:
+ [Метод оценки задач ICE и его соседи](#метод-оценки-задач-ice-и-его-соседи)

[Сайт](https://habr.com/ru/articles/538076/)

Дел у менеджеров по продукту всегда больше, чем ресурсов. Новые задачи приходят со всех сторон — от дизайнеров до маркетологов и CEO. Но если брать в работу все подряд, есть риск получить на выходе не то, что нужно для развития проекта, а то, что было «по фану» кому-то из коллег. 

Так, сооснователь KISSmetrics Хитен Ша [признался](https://producthabits.com/my-billion-dollar-mistake/), что его компания потеряла позиции на рынке именно из-за ошибок приоритизации. 

Как только у него появлялись новые идеи, подчиненные должны были бросить все и начать работу над ними. О вдумчивом распределении сил речь тогда не шла. 

Пока команда пыталась успеть за руководителем, конкуренты захватывали рынок и выпускали новые продукты.

Расскажем, как не допустить такого развития событий с помощью простых методик для приоритизации гипотез. Вообще говоря, эту тему часто поднимают на профильных конференциях и семинарах. Так, Мэтт Билотти, менеджер продукта в Drift, интерактивной маркетинговой платформы, которая [заняла](https://www.drift.com/about/newsroom/press-releases/deloitte-fast-500/) шестое место в рейтинге самых быстрорастущих компаний по версии Deloitte, [поделился](https://my.epicgrowth.ru/programs/driftmp4-b585c3) на Epic Growth SEASONS своим взглядом на то, как можно приоритизировать гипотезы на основе юнит-экономики. Он объяснил, в чем преимущества подхода и как его можно использовать в работе.

![Статья](/ABTesting/Pictures/002_024.jpeg)

[Содержание](#содержание)

<hr>

## Метод оценки задач ICE и его соседи

Некоторые команды расставляют приоритеты продуктовым задачам и гипотезам с помощью балльной системы — оценивают их по нескольким критериям:
+ степени влияния на продукт,
+ потенциальной скорости реализации,
+ степени уверенности в успехе гипотезы и др. 

Каждому фактору участники команды присваивают баллы — от одного до десяти — и находят среднее арифметическое. 

Допустим, если реализовать идею можно достаточно оперативно, критерий скорости реализации получит восемь-десять баллов. Итоговые показатели по всем гипотезам можно сравнить и определить приоритет.

Такой метод оценки называется __ICE:__ 
+ влияние (Impact),
+ уверенность (Confidence),
+ усилия (Effort). 

Он был придуман Шоном Эллисом, автором термина «Growth Hacking». 

Преимущество подхода — в его простоте: чтобы оценить задачу, достаточно присвоить значения от одного до десяти для каждого из факторов и вычислить ICE Score:

__ICE Score = Impact\*Confidence\*Effort.__

Недостаток метода — в его субъективности. 

Кто-то выполнит задачу за несколько часов, другие — потратят пару дней или неделю. 

Этот момент достаточно сложно оценить, поэтому команды, использующие метод оценки ICE, часто вязнут в обсуждениях и вкусовщине, а объективные показатели оставляют за бортом.

В отличие от него, так называемый RICE:
+ охват (Reach),
+ влияние (Impact),
+ уверенность в оценке (Confidence),
+ трудозатраты (Effort) — __более сбалансирован.__

RICE Score = (Reach*Impact*Confidence) / Effort.

Чтобы оценить фактор охвата, нужно рассчитать количество пользователей, которому придется столкнуться с изменениями в продукте. Фактор влияния обычно отражает ценность, которую фича приносит продукту. Для количественного выражения влияния используют шкалу множественного выбора (это такая модель выбора, когда нужно принять одно решение, но выбрать между тремя и более вариантами):

+ 3 — массовое влияние;
+ 2 — высокое;
+ 1 — среднее;
+ 0,5 — низкое;
+ 0,25 — минимальное.

Оценка фактора уверенности всегда вызывает много вопросов. Если остальные показатели можно оценить достаточно точно, то как оценить уверенность? И как снизить субъективное влияние в процессе утверждения приоритетов? 

Можно использовать [диаграмму](https://itamargilad.com/the-tool-that-will-help-you-choose-better-product-ideas/) Итамара Гилада, бывшего продакта в Google и Microsoft. 

![Diagram](/ABTesting/Pictures/002_025.png)

![Diagram](/ABTesting/Pictures/002_026.PNG)

На ней можно увидеть описание типичных фактов, на основе которых обычно рассчитывают значения параметра Confidence (личное мнение, экспертная оценка, идея коллеги, фича конкурента, результаты UX исследований, данные на основе интервью и др.). Предположим, ваша уверенность в успехе фичи основана на мнении кого-то из членов команды или личном мнении. Оценка этого фактора в таком случае будет около нуля.

Гипотезы, основанные на данных маркетинговых исследований, запросах пользователей, результатах юзабилити тестов получат низкий уровень уверенности (от 1 до 3). Если же вы, расставляя приоритеты задачам, делаете выводы на основе длительных исследований поведения пользователей, результатов запуска MVP, A/B-тестов и других достоверных данных, уровень уверенности может быть средним (от 3 до 7). 

![Статья](/ABTesting/Pictures/002_027.jpeg)

Вообще говоря, есть много разных подходов для приоритизации задач, все зависит запросов и целей команды. Какие-то компании даже придумывают свои методы. Так, в [Netflix используют DHM-подход](https://vc.ru/growth/153129-kakie-produktovye-resheniya-prinyal-netflix-v-2020-godu-i-vyros-do-185-mln-polzovateley), в котором гипотезы оценивают по трем критериям:

D — delight;

H — hard to copy;

M — margin.

На практике это означает, что нужно задать ряд вопросов относительно каждой гипотезы. Например, принесет ли прибыль ввод этой фичи? Будут ли пользователи удовлетворены изменениями? Насколько сложно будет конкурентам скопировать этот функционал? Для того, чтобы Netflix начали тестировать гипотезу, она должна соответствовать как минимум двум критериям DHM-модели, а лучше — всем трем.

На практике это означает, что нужно задать ряд вопросов относительно каждой гипотезы. 

Например, принесет ли прибыль ввод этой фичи? Будут ли пользователи удовлетворены изменениями? Насколько сложно будет конкурентам скопировать этот функционал? Для того, чтобы Netflix начали тестировать гипотезу, она должна соответствовать как минимум двум критериям DHM-модели, а лучше — всем трем.

![Статья](/ABTesting/Pictures/002_028.jpeg)

В блоге [Ducalis.io](http://ducalis.io/) мы нашли [классную схему](https://miro.com/app/board/o9J_kk67T8k=/), в которой можно выбрать фреймворк для приоритизации исходя из запросов конкретно вашей команды. Помимо уже известных  ICE, RICE, DHM, есть и другие подходы: REAN метод (Reach, Engage, Activate, Nurture), приоритизация на основе North Star метрики, матрица усилий (Effort Matrix), AARRR скоринг и др. 

[Содержание](#содержание)

<hr>

## Альтернативный подход

Если возвращаться к команде Drift, то Мэтт Билотти говорит, что отказ от ICE стал возможным благодаря поиску альтернатив. В этом ему помог Дариус Контрактор, который в течение четырех лет развивал Dropbox, руководил отделом роста в Facebook Messenger, а теперь работает Head of Growth в Airtable. Он рассказал Мэтту про подход EVELYN.

![Статья](/ABTesting/Pictures/002_029.png)

Drift вдохновились EVELYN и создали свою систему приоритизации. Суть в том, чтобы оценивать продуктовые гипотезы исходя из того, какую прибыль они могут принести компании. Для этого команда должна определить, что включает в себя весь [путь пользователя](https://ru.wikipedia.org/wiki/Клиентский_опыт#:~:text=Путь%20клиента%20от%20возникновения%20потребности,физические%2C%20телефон%2C%20digital) и рассчитать юнит-экономику на каждом шаге [воронки](https://ru.wikipedia.org/wiki/Воронка_продаж).

Когда вы построили весь путь пользователя и рассчитали экономику продукта, у вас появится перечень метрик. Это может выглядеть следующим образом:

UA — число привлеченных пользователей

Marketing costs — затраты на маркетинг

C1 — конверсия в первую покупку

Buyers — количество покупателей

AvP — средний чек

ARPC — средний доход на клиента (без учета маркетинговых затрат)

ARPU — средний доход на одного пользователя без учета маркетинговых затрат

ARPPU — доход с платящего пользователя за вычетом издержек 

CAC — стоимость привлечения клиента. 

CPA — стоимость одного привлеченного пользователя в начало воронки и др.

Более подробно о юнит-экономике можно почитать в блоге Даниила Ханина. В этой статье автор дает объяснение метрикам, которые упоминаются выше, можете начать с нее. 

Специалисты Drift предлагают выбрать десять-двадцать метрик в денежном выражении, на которые вы можете влиять, внося изменения в продукт или маркетинг. 

Затем сформулируйте гипотезы — благодаря каким действиям вы хотите увеличить или уменьшить выбранную метрику. Чтобы рассчитать, сколько гипотеза может принести компании, нужны следующие показатели:

— метрика, на которую вы хотите повлиять (в денежном выражении) (a);

— во сколько раз эксперимент может увеличить эту метрику (b);

— вероятность, что гипотеза сработает, в процентном выражении (c);

— количество дней, которое необходимо для реализации гипотезы (d).

Вот так выглядит формула:

Value per Day = (a * b * c) / d.

![Статья](/ABTesting/Pictures/002_030.png)

Например, метрика влияния — ARPU, или средний доход с пользователя, составляет $1. Вы должны определить, во сколько раз эксперимент увеличит вашу метрику. Например, вы предполагаете, что ARPU станет не $1, а $20. Значит, opportunity size будет равен двадцати.  

Предположим, вы уверены в успехе на 70%. Значит умножайте на 0,7. Если на реализацию идеи вам понадобится два дня, то полученное произведение цифр нужно разделить на два. Итого получаем: Value per Day = ($1*20*0,7)/2 = $7. 

Таким образом, Value per Day метрика отражает, сколько дохода фича может сгенерировать за N потраченных на ее реализацию рабочих дней. Так как в нашем примере Value per Day составил семь долларов, а эксперимент занял два дня, ежемесячно фича будет генерировать доход в четырнадцать долларов ($7*2) до тех пор, пока метрика влияния не изменятся (в ходе новых экспериментов, например). 

![Статья](/ABTesting/Pictures/002_031.jpeg)

После того, как вы получите значения Value per Day для всех гипотез, вам будет легче определиться с тем, за реализацию какой из них взяться в первую очередь. Так вы будете меньше времени тратить на обсуждение мнений и легче доносить ценность своих идей до коллег и руководителей. Если сделать документ с гипотезами открытым, любой человек в компании сможет внести свои идеи или посмотреть список задач в порядке приоритета. Так, у всех членов команды будет понимание того, какие гипотезы находятся в процессе тестирования и какой результат получен по уже проведенным экспериментам. Работа станет более предсказуемой.

[Содержание](#содержание)

<hr>

# Пример работы с методом ICE от менеджера продуктов Google и Microsoft

Работа с приоритетами — задача, требующая подготовки, опыта и рассмотрения множества технологий, научных подходов, а также авторских методов.

Эта статья – перевод материала с сайта Hackernoon.com. Ее автор предлагает применение собственного инструмента оценки приоритетов в рамках метода ICE Scoring. В этой статье детально описан подход и разобран простой и доступный пример, понятный любому менеджеру продукта.

![Статья 2](/ABTesting/Pictures/002_032.jpg)

[Itamar Gilad](https://twitter.com/ItamarGilad) — известный консультант в области управления продуктами и успешный спикер. В его многолетнем опыте — руководящие должности по управлению продуктами в Google, Microsoft и других известных компаниях. Мы предлагаем перевод его статьи:

Допустим, вы управляете продуктом для малого бизнеса и его клиентов. Ваша цель — улучшить степень вовлеченности и удержания клиентов. У вас на повестке две идеи:

+ Внедрение главной панели инструментов (dashboard), позволяющей владельцу бизнеса отслеживать статистику вовлечения и все тренды.
+ Чатбот (chatbot) для автоматизации общения с клиентами.

Идея с панелью инструментов возникала несколько раз в переговорах с клиентами, и вы чувствуете, что она имеет хороший потенциал, но существует риск того, что ее будут использовать только опытные пользователи.

Идея с чатботом нравится всей компании, и руководство довольно оптимистично воспринимает ее. Также фича выглядит выигрышной и для клиентов.

Что бы вы выбрали?

Этот вопросы приоритизации лежат в основе управления продуктом. Плата за неправильный выбор может оказаться весьма большой и включать стоимость разработки, деплоймента, обслуживания, а также другие внеплановые затраты.

Мы часто испытываем соблазн принять решение на основе неубедительных сигналов:
+ мнения большинства,
+ мнения начальников,
+ отраслевых тенденций и т. д. 

Но время показывает, что эти сигналы по точности находятся на уровне генератора случайных чисел.

Этот пост о том, как, на мой взгляд, находить лучшие идеи. Он состоит из трех частей:

+ Показатели ICE
+ Уровни доверия
+ Дополнительная проверка

[Содержание](#содержание)

<hr>

## ICE Scoring

[ICE Scoring](https://habr.com/company/hygger/blog/422131/) — это метод определения приоритетов, который впервые использовал Шон Эллис, известный своим активным участием в становлении таких компаний, как DropBox и Eventbrite, и продвижении термина Growth Hacking. ICE первоначально был предназначен для определения приоритетов в growth экспериментах, но вскоре стал использоваться для оценки любых идей.

В ICE вы оцениваете идеи таким образом:

![Статья 2](/ABTesting/Pictures/002_033.jpg)

+ __Влияние__ демонстрирует, насколько идея положительно повлияет на ключевой показатель, который вы пытаетесь улучшить.
+ __Легкость реализации__ или простота — это оценка того, сколько усилий и ресурсов требуется для реализации этой идеи.
+ __Уверенность__ демонстрирует, насколько вы уверены в оценках влияния и легкости реализации.

Значения в ICE оцениваются по шкале от 1 до 10, чтобы все факторы сбалансировано повлияли на итоговое число. Под значениями 1-10 можно подразумевать все что угодно, главное, чтобы значения были согласованы между собой.

![Статья 2](/ABTesting/Pictures/002_034.jpg)

Теперь давайте на примере посмотрим, как это работает.

## Сперва ICE

Итак, вы решили рассчитать баллы по ICE для двух идей: dashboard и чатбота. На этом раннем этапе вы используете грубые значения исключительно на основе собственной интуиции.

__Влияние(Impact)__ — вы предполагаете, что дэшборд значительно увеличит удержание пользователей, но только опытных, — вы даете 5 из 10. Чатбот, с другой стороны, может стать инновационным решением для многих клиентов, поэтому вы даете ему 8 из 10.

__Легкость реализации(Ease)__ — вы оцениваете, что для dashboard потребуется 10 человеко-недель, а для чат-бота — 20. Позже от команды вы получите более качественные оценки. Вы используете эту простую таблицу (выбранную вами и вашей командой) для преобразования вашей оценки в Ease:

![Статья 2](/ABTesting/Pictures/002_035.jpg)

Таким образом, панель инструментов получает значение Ease 4 из 10 и чатбот — значение 2.

## Вычисление уверенности

Существует только один способ рассчитать уверенность — это поиск подтверждающих доказательств. Для этого я создал инструмент, который можно увидеть ниже. В нем перечислены общие типы тестов и доказательств, которые могут быть у вас, и уровень уверенности, который они предоставляют: результаты тестов, дата лонча, собственная уверенность, тематическая поддержка, мнение других людей, данные рынка и др.

При использовании инструмента учитывайте, какие показатели у вас уже есть, сколько из них и что вам нужно, чтобы получить больше уверенности.

![Статья 2](/ABTesting/Pictures/002_036.jpg)

Если в вашем продукте или в отрасли возможна другая проверка доказательств, не стесняйтесь создавать свою собственную версию этого инструмента.

Вернемся к примеру, чтобы оценить инструмент в действии.

Поддержка доказательств для чатбота: личная уверенность (вы думаете, что это хорошая идея), тематическая поддержка (в индустрии также считают, что это хорошая идея) и мнение других (ваше начальство и коллеги считают это хорошей идеей). Это дает ему общее доверительное значение 0,1 из 10 или Near Zero Confidence. Инструмент явно не считает мнения надежным индикатором.

Что по поводу dashboard? Здесь личная уверенность (вы думаете, что это хорошая идея) и эпизодическая поддержка (несколько клиентов попросили об этом). Это фактически повышает его доверительное значение до 0,5 из 10, что является низким доверием. К сожалению, клиенты плохо прогнозируют свое будущее поведение.

ICE scoring в этом случае:

![Статья 2](/ABTesting/Pictures/002_037.jpg)

К этому моменту панель инструментов выглядит как лучшая идея, но наш инструмент показывает, что вы не вышли за пределы низкой уверенности. Для принятия решения пока просто недостаточно информации.

[Содержание](#содержание)

<hr>

[Содержание курса](/ABTesting/README.MD)


